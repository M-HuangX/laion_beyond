<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <meta name="description" content="LAION-Beyond: Reproducible Vision-Language Models Meet Concepts Out of Pre-Training">
  <meta property="og:title" content="LAION-Beyond: Reproducible Vision-Language Models Meet Concepts Out of Pre-Training"/>
  <meta property="og:description" content="Exploring how vision-language models generalize to concepts not seen during pre-training"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/laion_beyond_banner.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="vision-language models, CLIP, out-of-pre-training generalization, few-shot learning, zero-shot learning, LAION-Beyond">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>LAION-Beyond: Reproducible Vision-Language Models Meet Concepts Out of Pre-Training</title>
  <link rel="icon" type="image/png" href="https://github.githubassets.com/favicons/favicon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">LAION-Beyond: Reproducible Vision-Language Models Meet Concepts Out of Pre-Training</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=RC-LN4QAAAAJ&hl=en" target="_blank">Ziliang Chen</a><sup>*,1</sup>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/mhuangx/" target="_blank">Xin Huang</a><sup>*,2,4</sup>,</span>
                  <span class="author-block">
                    <a href="" target="_blank">Xiaoxuan Fan</a><sup>3</sup>,</span>
                    <span class="author-block">
                      <a href="https://kezewang.com/" target="_blank">Keze Wang</a><sup>2</sup>,</span>
                      <span class="author-block">
                        <a href="" target="_blank">Yuyu Zhou</a><sup>3</sup>,</span>
                        <span class="author-block">
                          <a href="https://scholar.google.com/citations?user=v4JiSqsAAAAJ&hl=en" target="_blank">Quanlong Guan</a><sup>3</sup>,</span>
                          <span class="author-block">
                            <a href="http://www.linliang.net/" target="_blank">Liang Lin</a><sup>2,1</sup>
                  </span>
                  </div>
              
                  <div class="is-size-5 publication-authors">
                    <span class="author-block" style="font-size: 0.9em;"><sup>1</sup>Peng Cheng Laboratory, <sup>2</sup>Sun Yat-sen University, <sup>3</sup>Jinan University, <sup>4</sup>École polytechnique fédérale de Lausanne(EPFL)<br>CVPR 2025</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution, Listed Alphabetically By Last Name</small></span>
              <!--       <span class="eql-cntrb"><small><br><sup>†</sup> Corresponding Author</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://openreview.net/pdf?id=hOiG8R14AH" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/M-HuangX/LAION-Beyond" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                    <a href="https://arxiv.org/abs/12345" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv - Coming Soon...</span>
                  </a>
                </span>

                <!-- Dataset Link -->
                <span class="link-block">
                  <a href="#" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Contrastive Language-Image Pre-training (CLIP) models, as a milestone of modern multimodal intelligence, have attracted significant research interest regarding their generalization mechanisms. While existing studies have been limited to the scope of pre-training knowledge, they have hardly addressed the model's generalization to countless open-world concepts absent from the pre-training regime. This paper investigates this Out-of-Pre-training (OOP) generalization problem from a holistic perspective. We propose the LAION-Beyond benchmark to isolate the evaluation of OOP concepts from pre-training knowledge, focusing on OpenCLIP and its reproducible variants derived from LAION datasets. Empirical analysis shows that despite image features of OOP concepts exhibiting significant category margins, their zero-shot transfer significantly fails due to poor image-text alignment. To address this, we elaborate the "name-tuning" methodology with its theoretical merits for OOP generalization, and propose few-shot name learning (FSNL) and zero-shot name learning (ZSNL) algorithms to achieve OOP generalization in a data-efficient manner. Their superiority has been further verified in our comprehensive experiments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Key Highlights Section -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Key Highlights</h2>
    <div class="columns is-centered">
      <div class="column">
        <div class="card">
          <div class="card-content">
            <p class="title is-4">First Systematic Study</p>
            <p class="content">First research to systematically explore vision-language models' generalization to concepts absent from pre-training data</p>
          </div>
        </div>
      </div>
      <div class="column">
        <div class="card">
          <div class="card-content">
            <p class="title is-4">Novel Benchmark</p>
            <p class="content">LAION-Beyond: the first multi-domain benchmark specifically designed to evaluate OOP concept generalization</p>
          </div>
        </div>
      </div>
      <div class="column">
        <div class="card">
          <div class="card-content">
            <p class="title is-4">Novel Algorithms</p>
            <p class="content">FSNL and ZSNL: innovative approaches to effectively solve the OOP generalization problem</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Key Highlights Section -->

<!-- Research Background Section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Research Background</h2>
    <div class="content has-text-justified">
      <p>
        Modern vision-language models (like CLIP) have demonstrated remarkable zero-shot and few-shot learning capabilities through large-scale image-text pair pre-training. However, how these models perform when faced with concepts never encountered during pre-training remains a critical yet under-explored question.
      </p>
      <p>
        <strong>Research Distinction:</strong> We distinguish between IP concepts (In-Pre-training) that appear in pre-training data and OOP concepts (Out-of-Pre-training) that are absent from pre-training data.
      </p>

      <!-- Placeholder for Figure 1: IP vs OOP concept comparison -->
      <div class="has-text-centered">
        <figure class="image">
          <img src="static/images/Figure1_OOP_IP_difference.jpg" alt="Comparison between IP and OOP generalization">
          <figcaption>Figure 1: Comparison between IP and OOP generalization. The former evaluates OpenCLIP's generalization with visual concepts seen in pre-training phases, whereas the latter justifies its generalization through the concepts absent during pre-training.</figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>
<!-- End Research Background Section -->

<!-- LAION-Beyond Benchmark Section -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">The LAION-Beyond Benchmark</h2>
    <div class="content has-text-justified">
      <p>
        We constructed the LAION-Beyond benchmark as the first dedicated dataset for evaluating vision-language models' generalization to Out-of-Pre-training concepts:
      </p>
      <ul>
        <li><strong>Scale:</strong> 106,052 images across 674 OOP concepts and 51,330 images across 324 IP concepts</li>
        <li><strong>Coverage:</strong> Spans 9 diverse domains including Plants & Fungi, Insects & Spiders, Animals, Pokemon, FolkArt, Landmark, Attire, Food, and Architecture</li>
        <li><strong>Multiple Versions:</strong> Provides subsets corresponding to LAION-400M, LAION-2B, and LAION-5B to support neural scaling law research</li>
      </ul>

      <!-- Placeholder for Figure 2: LAION-Beyond dataset statistics -->
      <div class="has-text-centered">
        <figure class="image">
          <img src="static/images/Figure2a_LAION_Beyond_Distribution.png" alt="Statistics of OOP and IP concepts in LAION-Beyond">
          <figcaption>Figure 2a: The statistics of OOP and IP concepts and their images in LAION-Beyond (400M), (2B), and (5B).</figcaption>
        </figure>
        <figure class="image">
            <img src="static/images/Figure2b_Image_Counts_per_category.png" alt="Statistics of OOP and IP concepts in LAION-Beyond">
            <figcaption>Figure 2b: The statistics of train, val, test split in LAION-Beyond (400M).</figcaption>
          </figure>
      </div>
    </div>
  </div>
</section>
<!-- End LAION-Beyond Benchmark Section -->

<!-- Key Findings Section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Key Findings</h2>
    
    <!-- Finding 1 -->
    <div class="box">
      <h3 class="title is-4">1. Strong Image Feature Representation for OOP Concepts</h3>
      <div class="content has-text-justified">
        <p>
          OpenCLIP's image encoder can extract features with clear clustering boundaries for OOP concepts, even though these concepts never appeared during pre-training:
        </p>
        <ul>
          <li>Cluster visualization shows distinct class boundaries for OOP concept image features</li>
          <li>The clustering accuracy gap between OOP and IP concepts is less than 3% across most domains</li>
        </ul>

        <!-- Placeholder for Figure 3 and Table 1 -->
        <div class="has-text-centered">
            <figure class="image">
                <img src="static/images/Figure3_tSNE.png" alt="t-SNE visualization of image features">
                <figcaption>Figure 3: The t-SNE visualization for (a) image features from 20 OOP classes drawn from Plants & Fungi; (b) image features from 10 OOP classes and 10 IP classes.</figcaption>
            </figure>

            <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
              <thead>
                <tr>
                  <th></th>
                  <th>Anim</th>
                  <th>Arch</th>
                  <th>Atti</th>
                  <th>Folk</th>
                  <th>Food</th>
                  <th>Insect</th>
                  <th>Ladmk</th>
                  <th>Plant</th>
                  <th>Pokem</th>
                  <th>Avg</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>IP classes</td>
                  <td>40.27</td>
                  <td>91.04</td>
                  <td>82.09</td>
                  <td>78.02</td>
                  <td>81.72</td>
                  <td>50.44</td>
                  <td>93.01</td>
                  <td>55.71</td>
                  <td>34.07</td>
                  <td>68.15</td>
                </tr>
                <tr>
                  <td>OOP classes</td>
                  <td>37.27</td>
                  <td>81.06</td>
                  <td>68.92</td>
                  <td>76.60</td>
                  <td>80.65</td>
                  <td>48.30</td>
                  <td>86.39</td>
                  <td>53.17</td>
                  <td>35.80</td>
                  <td>63.13</td>
                </tr>
                <tr>
                  <td>IP-OOP gap</td>
                  <td>3.00</td>
                  <td>10.02</td>
                  <td>13.83</td>
                  <td>1.42</td>
                  <td>1.07</td>
                  <td>2.14</td>
                  <td>6.62</td>
                  <td>2.54</td>
                  <td>1.73</td>
                  <td>5.02</td>
                </tr>
              </tbody>
              <caption>Table 1. The normalized clustering accuracy across features extracted from OOP-class test images and IP-class test images across 9 domains, respectively.</caption>
            </table>
          </div>
      </div>
    </div>

    <!-- Finding 2 -->
    <div class="box">
      <h3 class="title is-4">2. Image-Text Alignment Failure</h3>
      <div class="content has-text-justified">
        <p>
          Despite strong image encoding capabilities, OpenCLIP fails to achieve cross-modal alignment for OOP concepts:
        </p>
        <ul>
          <li>Zero-shot classification accuracy for OOP concepts is significantly lower than for IP concepts</li>
          <li>The image-text alignment issue persists even with increasing pre-training data scale</li>
          <li>Root cause: Token embeddings for OOP concepts were not initialized with any image-text alignment during pre-training</li>
        </ul>

        <!-- Placeholder for Figure 4 -->
        <div class="has-text-centered">
          <figure class="image">
            <img src="static/images/Figure4_zs_openclip_LAION_Beyond.png" alt="Zero-shot performance comparison">
            <figcaption>Figure 4: OpenCLIP's zero-shot inference accuracy on OOP and IP classes in LAION-Beyond (400M).</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Key Findings Section -->

<!-- Methodology Section -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Our Approach</h2>
    <div class="content has-text-justified">
      <p>
        Based on our findings, we developed two novel algorithms to address the OOP generalization problem:
      </p>
    </div>

    <!-- FSNL -->
    <div class="box">
      <h3 class="title is-4">Few-Shot Name Learning (FSNL)</h3>
      <div class="content has-text-justified">
        <p>For scenarios with a few image-text pairs for OOP concepts:</p>
        <ul>
          <li>Optimizes only the name embeddings of OOP concepts, keeping other parameters fixed</li>
          <li>Enhances training with context augmentation through similar concept shuffling</li>
          <li>Combines theoretical guarantees with empirical validation</li>
        </ul>
      </div>
    </div>

    <!-- ZSNL -->
    <div class="box">
      <h3 class="title is-4">Zero-Shot Name Learning (ZSNL)</h3>
      <div class="content has-text-justified">
        <p>For scenarios with no image-text pairs for OOP concepts:</p>
        <ul>
          <li>Leverages Novel Class Discovery (NCD) techniques to guide OOP class image clustering</li>
          <li>Implements image-text bipartite graph matching</li>
          <li>Optimizes OOP name embeddings using high-confidence samples</li>
        </ul>
      </div>
    </div>
  </div>
</section>
<!-- End Methodology Section -->

<!-- Experimental Results Section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Experimental Results</h2>

    <!-- OOP Few-Shot Learning -->
    <div class="box">
      <h3 class="title is-4">OOP Few-Shot Learning</h3>
      <div class="content has-text-justified">
        <p>
          FSNL outperforms all baseline methods across 8 domains (except Food), with performance continuously improving as sample count increases.
        </p>

        <!-- Placeholder for Figure 5 -->
        <div class="has-text-centered">
          <figure class="image">
            <img src="static/images/Figure5_FSNL_compare_with_other_method.png" alt="Few-shot learning performance comparison">
            <figcaption>Figure 5: OOP few-shot learning performances (1,2,4,8,16 shots) of different baselines in the test sets of Animals, Landmark, and Pokemon across 9 domains in LAION-Beyond (400M).</figcaption>
          </figure>
        </div>
      </div>
    </div>

    <!-- OOP to IP Balance -->
    <div class="box">
      <h3 class="title is-4">OOP and IP Concept Balance</h3>
      <div class="content has-text-justified">
        <p>
          FSNL maintains high recognition rates for OOP concepts while preserving original performance on IP concepts, achieving the best of both worlds.
        </p>

        <!-- Replace the image placeholder with this HTML table -->
<div class="has-text-centered" style="overflow-x: auto;">
    <table class="table is-bordered is-narrow is-hoverable is-fullwidth"  style="font-size: 0.75rem; line-height: 1.2;">
      <thead>
        <tr>
          <th rowspan="2">Baselines</th>
          <th rowspan="2">Metric</th>
          <th colspan="10" style="text-align: center;">Domains</th>
          <th rowspan="2">Extra subnet</th>
        </tr>
        <tr>
          <th>Animals</th>
          <th>Architecture</th>
          <th>Attire</th>
          <th>FolkArt</th>
          <th>Food</th>
          <th>Insects_Spider</th>
          <th>Landmark</th>
          <th>Plants_Fungi</th>
          <th>Pokemon</th>
          <th>Avg</th>
        </tr>
      </thead>
      <tbody>
        <!-- OpenCLIP -->
        <tr>
          <td rowspan="3">OpenCLIP</td>
          <td style="background-color: #f8d7f8;">OOP</td>
          <td style="background-color: #f8d7f8;">19.70</td>
          <td style="background-color: #f8d7f8;">22.49</td>
          <td style="background-color: #f8d7f8;">16.18</td>
          <td style="background-color: #f8d7f8;">27.07</td>
          <td style="background-color: #f8d7f8;">8.88</td>
          <td style="background-color: #f8d7f8;">16.86</td>
          <td style="background-color: #f8d7f8;">25.65</td>
          <td style="background-color: #f8d7f8;">15.71</td>
          <td style="background-color: #f8d7f8;">15.47</td>
          <td style="background-color: #f8d7f8;">18.67</td>
          <td rowspan="3">No</td>
        </tr>
        <tr>
          <td style="background-color: #d7f8f8;">IP</td>
          <td style="background-color: #d7f8f8;"><strong>41.66</strong></td>
          <td style="background-color: #d7f8f8;"><strong>48.60</strong></td>
          <td style="background-color: #d7f8f8;"><strong>64.57</strong></td>
          <td style="background-color: #d7f8f8;"><strong>49.67</strong></td>
          <td style="background-color: #d7f8f8;"><strong>56.93</strong></td>
          <td style="background-color: #d7f8f8;"><strong>33.28</strong></td>
          <td style="background-color: #d7f8f8;"><strong>93.41</strong></td>
          <td style="background-color: #d7f8f8;"><strong>33.71</strong></td>
          <td style="background-color: #d7f8f8;"><strong>58.58</strong></td>
          <td style="background-color: #d7f8f8;"><strong>53.38</strong></td>
        </tr>
        <tr>
          <td>H-mean</td>
          <td>26.75</td>
          <td>30.75</td>
          <td>25.88</td>
          <td>35.04</td>
          <td>15.36</td>
          <td>22.38</td>
          <td>40.25</td>
          <td>21.43</td>
          <td>24.48</td>
          <td>26.92</td>
        </tr>
        
        <!-- CoOp -->
        <tr>
          <td rowspan="3">CoOp</td>
          <td style="background-color: #f8d7f8;">OOP</td>
          <td style="background-color: #f8d7f8;">38.31</td>
          <td style="background-color: #f8d7f8;">76.54</td>
          <td style="background-color: #f8d7f8;">60.28</td>
          <td style="background-color: #f8d7f8;">68.35</td>
          <td style="background-color: #f8d7f8;">61.67</td>
          <td style="background-color: #f8d7f8;">47.18</td>
          <td style="background-color: #f8d7f8;">85.24</td>
          <td style="background-color: #f8d7f8;">48.25</td>
          <td style="background-color: #f8d7f8;">39.24</td>
          <td style="background-color: #f8d7f8;">58.34</td>
          <td rowspan="3">No</td>
        </tr>
        <tr>
          <td style="background-color: #d7f8f8;">IP</td>
          <td style="background-color: #d7f8f8;">26.56</td>
          <td style="background-color: #d7f8f8;">46.43</td>
          <td style="background-color: #d7f8f8;">43.29</td>
          <td style="background-color: #d7f8f8;">42.04</td>
          <td style="background-color: #d7f8f8;">32.48</td>
          <td style="background-color: #d7f8f8;">17.69</td>
          <td style="background-color: #d7f8f8;">86.54</td>
          <td style="background-color: #d7f8f8;">16.67</td>
          <td style="background-color: #d7f8f8;">32.45</td>
          <td style="background-color: #d7f8f8;">38.24</td>
        </tr>
        <tr>
          <td>H-mean</td>
          <td>31.37</td>
          <td>57.8</td>
          <td>50.39</td>
          <td>52.06</td>
          <td>42.55</td>
          <td>25.73</td>
          <td>85.89</td>
          <td>24.78</td>
          <td>35.52</td>
          <td>45.12</td>
        </tr>
        
        <!-- CoCoOp -->
        <tr>
          <td rowspan="3">CoCoOp</td>
          <td style="background-color: #f8d7f8;">OOP</td>
          <td style="background-color: #f8d7f8;">23.82</td>
          <td style="background-color: #f8d7f8;">38.53</td>
          <td style="background-color: #f8d7f8;">29.58</td>
          <td style="background-color: #f8d7f8;">31.29</td>
          <td style="background-color: #f8d7f8;">19.94</td>
          <td style="background-color: #f8d7f8;">23.16</td>
          <td style="background-color: #f8d7f8;">45.81</td>
          <td style="background-color: #f8d7f8;">20.35</td>
          <td style="background-color: #f8d7f8;">18.10</td>
          <td style="background-color: #f8d7f8;">27.84</td>
          <td rowspan="3">Yes</td>
        </tr>
        <tr>
          <td style="background-color: #d7f8f8;">IP</td>
          <td style="background-color: #d7f8f8;">36.82</td>
          <td style="background-color: #d7f8f8;">41.30</td>
          <td style="background-color: #d7f8f8;">39.40</td>
          <td style="background-color: #d7f8f8;">33.19</td>
          <td style="background-color: #d7f8f8;">36.15</td>
          <td style="background-color: #d7f8f8;">30.76</td>
          <td style="background-color: #d7f8f8;">85.27</td>
          <td style="background-color: #d7f8f8;">28.29</td>
          <td style="background-color: #d7f8f8;">37.22</td>
          <td style="background-color: #d7f8f8;">40.93</td>
        </tr>
        <tr>
          <td>H-mean</td>
          <td>28.93</td>
          <td>39.87</td>
          <td>33.79</td>
          <td>32.21</td>
          <td>25.7</td>
          <td>26.42</td>
          <td>59.6</td>
          <td>23.67</td>
          <td>24.36</td>
          <td>32.73</td>
        </tr>
        
        <!-- CLIP-Adapter -->
        <tr>
          <td rowspan="3">CLIP-Adapter</td>
          <td style="background-color: #f8d7f8;">OOP</td>
          <td style="background-color: #f8d7f8;">42.80</td>
          <td style="background-color: #f8d7f8;">81.40</td>
          <td style="background-color: #f8d7f8;">73.72</td>
          <td style="background-color: #f8d7f8;">78.20</td>
          <td style="background-color: #f8d7f8;">83.48</td>
          <td style="background-color: #f8d7f8;">51.00</td>
          <td style="background-color: #f8d7f8;">92.03</td>
          <td style="background-color: #f8d7f8;">54.17</td>
          <td style="background-color: #f8d7f8;">63.82</td>
          <td style="background-color: #f8d7f8;">68.69</td>
          <td rowspan="3">Yes</td>
        </tr>
        <tr>
          <td style="background-color: #d7f8f8;">IP</td>
          <td style="background-color: #d7f8f8;">35.78</td>
          <td style="background-color: #d7f8f8;">46.60</td>
          <td style="background-color: #d7f8f8;">57.43</td>
          <td style="background-color: #d7f8f8;">44.01</td>
          <td style="background-color: #d7f8f8;">52.31</td>
          <td style="background-color: #d7f8f8;">23.86</td>
          <td style="background-color: #d7f8f8;">89.64</td>
          <td style="background-color: #d7f8f8;">22.68</td>
          <td style="background-color: #d7f8f8;">48.30</td>
          <td style="background-color: #d7f8f8;">46.73</td>
        </tr>
        <tr>
          <td>H-mean</td>
          <td>38.98</td>
          <td>59.27</td>
          <td>64.56</td>
          <td>56.32</td>
          <td>64.32</td>
          <td>32.51</td>
          <td>90.82</td>
          <td>31.97</td>
          <td>54.99</td>
          <td>54.86</td>
        </tr>
        
        <!-- Learning-to-Name -->
        <tr>
          <td rowspan="3">Learning-to-Name</td>
          <td style="background-color: #f8d7f8;">OOP</td>
          <td style="background-color: #f8d7f8;">26.18</td>
          <td style="background-color: #f8d7f8;">41.64</td>
          <td style="background-color: #f8d7f8;">43.24</td>
          <td style="background-color: #f8d7f8;">54.11</td>
          <td style="background-color: #f8d7f8;">44.18</td>
          <td style="background-color: #f8d7f8;">31.21</td>
          <td style="background-color: #f8d7f8;">50.13</td>
          <td style="background-color: #f8d7f8;">24.76</td>
          <td style="background-color: #f8d7f8;">37.99</td>
          <td style="background-color: #f8d7f8;">39.27</td>
          <td rowspan="3">Yes</td>
        </tr>
        <tr>
          <td style="background-color: #d7f8f8;">IP</td>
          <td style="background-color: #d7f8f8;">32.86</td>
          <td style="background-color: #d7f8f8;">40.16</td>
          <td style="background-color: #d7f8f8;">51.24</td>
          <td style="background-color: #d7f8f8;">39.89</td>
          <td style="background-color: #d7f8f8;">42.17</td>
          <td style="background-color: #d7f8f8;">28.90</td>
          <td style="background-color: #d7f8f8;">88.21</td>
          <td style="background-color: #d7f8f8;">26.05</td>
          <td style="background-color: #d7f8f8;">45.11</td>
          <td style="background-color: #d7f8f8;">43.84</td>
        </tr>
        <tr>
          <td>H-mean</td>
          <td>29.14</td>
          <td>40.89</td>
          <td>46.90</td>
          <td>45.92</td>
          <td>43.15</td>
          <td>30.01</td>
          <td>63.93</td>
          <td>25.39</td>
          <td>41.25</td>
          <td>40.73</td>
        </tr>
        
        <!-- FSNL(ours) -->
        <tr>
          <td rowspan="3">FSNL(ours)</td>
          <td style="background-color: #f8d7f8;">OOP</td>
          <td style="background-color: #f8d7f8;"><strong>51.77</strong></td>
          <td style="background-color: #f8d7f8;"><strong>88.03</strong></td>
          <td style="background-color: #f8d7f8;"><strong>80.47</strong></td>
          <td style="background-color: #f8d7f8;"><strong>86.23</strong></td>
          <td style="background-color: #f8d7f8;"><strong>90.85</strong></td>
          <td style="background-color: #f8d7f8;"><strong>65.03</strong></td>
          <td style="background-color: #f8d7f8;"><strong>95.57</strong></td>
          <td style="background-color: #f8d7f8;"><strong>63.85</strong></td>
          <td style="background-color: #f8d7f8;"><strong>77.88</strong></td>
          <td style="background-color: #f8d7f8;"><strong>77.74</strong></td>
          <td rowspan="3">No</td>
        </tr>
        <tr>
          <td style="background-color: #d7f8f8;">IP</td>
          <td style="background-color: #d7f8f8;"><strong>41.66</strong></td>
          <td style="background-color: #d7f8f8;"><strong>48.60</strong></td>
          <td style="background-color: #d7f8f8;"><strong>64.57</strong></td>
          <td style="background-color: #d7f8f8;"><strong>49.67</strong></td>
          <td style="background-color: #d7f8f8;"><strong>56.93</strong></td>
          <td style="background-color: #d7f8f8;"><strong>33.28</strong></td>
          <td style="background-color: #d7f8f8;"><strong>93.41</strong></td>
          <td style="background-color: #d7f8f8;"><strong>33.71</strong></td>
          <td style="background-color: #d7f8f8;"><strong>58.58</strong></td>
          <td style="background-color: #d7f8f8;"><strong>53.38</strong></td>
        </tr>
        <tr>
          <td>H-mean</td>
          <td><strong>46.17</strong></td>
          <td><strong>62.63</strong></td>
          <td><strong>71.65</strong></td>
          <td><strong>63.03</strong></td>
          <td><strong>70.0</strong></td>
          <td><strong>44.03</strong></td>
          <td><strong>94.48</strong></td>
          <td><strong>44.12</strong></td>
          <td><strong>68.87</strong></td>
          <td><strong>62.55</strong></td>
        </tr>
      </tbody>
      <caption>Table 2. OOP-to-IP open-vocabulary prediction. <strong>OOP</strong>, <strong>IP</strong>, and <strong>H-mean</strong> represent the accuracies of the models trained for OOP few-shot learning (4-shot), their accuracies in IP image dataset, and their compound Harmonic mean score, respectively (best viewed in color).</caption>
    </table>
  </div>
      </div>
    </div>

    <!-- Open-World Vocabulary Transfer -->
    <div class="box">
      <h3 class="title is-4">Open-World Vocabulary Transfer</h3>
      <div class="content has-text-justified">
        <p>
          In evaluations with mixed OOP and IP concepts, FSNL significantly outperforms all baseline methods.
        </p>

        <!-- Replace the image placeholder with this HTML table -->
        <div class="has-text-centered" style="overflow-x: auto;">
            <table class="table is-bordered is-narrow is-hoverable is-fullwidth">
            <caption>Table 3. The open-world transfer results (ACC across all OOP-class test images and IP-class images) across 9 domains. <em>Linear Probe</em> and <em>TaskRes</em> have been excluded due to their failure to transfer the training vocabulary. Δ indicates the absolute ratio that FSNL exceeds the second best.</caption>
            <thead>
                <tr>
                <th></th>
                <th>Anim</th>
                <th>Arch</th>
                <th>Atti</th>
                <th>Folk</th>
                <th>Food</th>
                <th>Insect</th>
                <th>Ladmk</th>
                <th>Plant</th>
                <th>Pokem</th>
                <th>Avg</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                <td>OpenCLIP</td>
                <td>19.40</td>
                <td>25.21</td>
                <td>25.23</td>
                <td>27.75</td>
                <td>18.86</td>
                <td>16.43</td>
                <td>46.10</td>
                <td>16.47</td>
                <td>26.89</td>
                <td>24.7</td>
                </tr>
                <tr>
                <td>CoOp</td>
                <td>23.99</td>
                <td>57.93</td>
                <td>43.85</td>
                <td>52.33</td>
                <td>32.63</td>
                <td>26.37</td>
                <td>80.07</td>
                <td>27.21</td>
                <td>22.78</td>
                <td>40.8</td>
                </tr>
                <tr>
                <td>CoCoOp</td>
                <td>18.86</td>
                <td>29.40</td>
                <td>23.78</td>
                <td>22.54</td>
                <td>17.09</td>
                <td>17.78</td>
                <td>50.14</td>
                <td>16.93</td>
                <td>18.38</td>
                <td>23.88</td>
                </tr>
                <tr>
                <td>CLIP-Adap</td>
                <td>29.51</td>
                <td>64.60</td>
                <td>58.92</td>
                <td>59.98</td>
                <td>64.14</td>
                <td>29.23</td>
                <td>85.05</td>
                <td>32.89</td>
                <td>54.47</td>
                <td>53.2</td>
                </tr>
                <tr>
                <td>L2Name</td>
                <td>21.78</td>
                <td>33.02</td>
                <td>25.26</td>
                <td>27.09</td>
                <td>25.14</td>
                <td>21.13</td>
                <td>67.05</td>
                <td>22.89</td>
                <td>26.47</td>
                <td>29.98</td>
                </tr>
                <tr>
                <td><strong>FSNL(ours)</strong></td>
                <td><strong>36.35</strong></td>
                <td><strong>71.00</strong></td>
                <td><strong>63.75</strong></td>
                <td><strong>69.27</strong></td>
                <td><strong>68.09</strong></td>
                <td><strong>39.70</strong></td>
                <td><strong>92.54</strong></td>
                <td><strong>40.53</strong></td>
                <td><strong>65.29</strong></td>
                <td><strong>60.72</strong></td>
                </tr>
                <tr>
                <td>Δ</td>
                <td>+6.84</td>
                <td>+6.40</td>
                <td>+5.42</td>
                <td>+9.29</td>
                <td>+3.95</td>
                <td>+10.47</td>
                <td>+7.49</td>
                <td>+7.64</td>
                <td>+10.82</td>
                <td>+7.52</td>
                </tr>
            </tbody>
            </table>
        </div>
      </div>
    </div>

    <!-- OOP Zero-Shot Learning -->
    <div class="box">
      <h3 class="title is-4">OOP Zero-Shot Learning</h3>
      <div class="content has-text-justified">
        <p>
          ZSNL substantially outperforms baseline methods in zero-shot learning for OOP concepts across 13 domains.
        </p>

        <!-- Placeholder for Table 4 -->
        <div class="has-text-centered">
          <figure class="image">
            <img src="[Table 4: OOP zero-shot learning accuracy comparison]" alt="OOP zero-shot learning accuracy">
            <figcaption>Table 4: Zero-shot learning ACC (%) in OOP classes drawn from domains in LAION-Beyond (400M) and (5B).</figcaption>
          </figure>
        </div>
      </div>
    </div>

    <!-- Neural Scaling Law Performance -->
    <div class="box">
      <h3 class="title is-4">FSNL Performance Across Model Scales</h3>
      <div class="content has-text-justified">
        <p>
          FSNL demonstrates consistent performance improvements across different model scales, including larger models and different CLIP variants (OpenAI CLIP, EVA-CLIP).
        </p>

        <!-- Placeholder for Neural Scaling Law Figure -->
        <div class="has-text-centered">
          <figure class="image">
            <img src="static/images/Figure6_FSNL_all_scale_models.png" alt="FSNL performance across model scales">
            <figcaption>Figure 6: FSNL performance under neural scaling law, demonstrating effective improvement across larger models and different CLIP variants (OpenAI, EVA). The light-colored circles represent the Zero-shot results of the corresponding models on OOP data of LAION Beyond, while the dark-colored circles represent the results of the corresponding models after FSNL training.</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Experimental Results Section -->

<!-- Impact Section -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Impact and Significance</h2>
    <div class="content has-text-justified">
      <p>
        Our research fills a critical gap in vision-language model research, with far-reaching impacts:
      </p>
      <ul>
        <li><strong>Theoretical Contribution:</strong> First theoretical framework for the OOP generalization problem</li>
        <li><strong>Practical Value:</strong> Efficient algorithms for handling new concepts in the open world</li>
        <li><strong>Future Direction:</strong> Paving the way for truly open-world multimodal systems</li>
      </ul>
      <p>
        The LAION-Beyond benchmark and our two algorithms not only reveal the capabilities and limitations of vision-language models with OOP concepts but also provide effective solutions for OOP generalization, offering significant value for advancing open-world multimodal AI.
      </p>
    </div>
  </div>
</section>
<!-- End Impact Section -->

<!--BibTex citation -->
<section class="section hero is-light" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{anonymous2025laionbeyond,
  title={Reproducible Vision-Language Models Meet Concepts Out of Pre-Training},
  author={Anonymous},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2025}
}</code></pre>
  </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
